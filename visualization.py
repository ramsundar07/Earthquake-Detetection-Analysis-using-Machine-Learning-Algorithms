df=joblib.load('df.joblib')
data=spark.createDataFrame(df)
data.head(5)
df_max= data.groupBy( 'Year') .max( 'Magnitude').withColumnRenamed ('max (Magnitude)', 'Max_Magnitude') 
df_avg = data.groupBy( 'Year').avg('Magnitude').withColumnRenamed ('avg (Magnitude)', 'Avg_Magnitude')
df_quake_freq = data.groupBy( 'Year').count().withColumnRenamed ('count', 'Counts') 
df_quake_freq.show(5)
df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year']) 
df_quake_freq = df_quake_freq.orderBy(asc('Year')) 
df_quake_freq.show(5)
df_quake_freq=df_quake_freq.toPandas()